{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n",
    "\n",
    "# <font color=\"red\">Model Training with Data Flow and ADS</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Overview:\n",
    "\n",
    "Train your models using ``ads``, ``OCI Data Flow``, and ``pyspark``'s MLLib. Run through an example training a model locally, then submitting that script to ``OCI Data Flow`` using ``ads``' Jobs API.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "\n",
    "- <a href='#intro'>Introduction</a>\n",
    "    - <a href=\"#setup\">Setup</a>\n",
    "- <a href='#local'>Establishing a Model Tuning Script Locally</a>\n",
    "    - <a href=\"#data\">Source Data</a>\n",
    "    - <a href=\"#build\">Build a Model on Spark</a>\n",
    "- <a href=\"#remote\">Running on Data Flow</a>\n",
    "- <a href=\"#cleanup\">Clean Up</a>\n",
    "- <a href=\"#reference\">References</a>\n",
    "\n",
    "---\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials under your agreement with Oracle.    \n",
    "\n",
    "---\n",
    "\n",
    "The notebook is compatible with the following [Data Science conda environments](https://docs.oracle.com/en-us/iaas/data-science/using/conda_environ_list.htm):\n",
    "\n",
    "* [PySpark 3.2 and Data Flow](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8 (version 2.0)\n",
    "\n",
    "---\n",
    "\n",
    "As of September 2022, the latest PySpark env is `pyspark32_p38_cpu_v2`, which can be installed by running:\n",
    "```\n",
    "odsc conda install -s pyspark32_p38_cpu_v2\n",
    "```\n",
    "in the terminal. To check the most up to date version, visit the Environment Explorer from the Launcher Tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You may need to install additional dependencies: `conda install -c conda-forge requests aiohttp`\n",
    "\n",
    "import fsspec\n",
    "import os\n",
    "import tempfile\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from ads.model.framework.spark_model import SparkPipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# Introduction\n",
    "\n",
    "Model training with MLLib can be scaled up to extremely large cluster sizes, making model training arbitraily quick. ``ads`` can manage creating the Spark CLuster (using Data Flow), and expose full configurability to the Notebook user..\n",
    "\n",
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "First we need to setup the details of the environment we want to use. These are listed out in the following cell with brackets on either said \"<>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compartment_id = \"<compartment_id>\"  # os.environ[\"NB_SESSION_COMPARTMENT_OCID\"] # For the OCI Notebook Session \n",
    "logs_bucket_uri = \"<logs_bucket_uri>\"\n",
    "conda_packs_bucket = \"<conda_packs_bucket>\"\n",
    "conda_packs_namespace = \"<conda_packs_namespace>\"\n",
    "script_bucket_uri = \"oci://<script_bucket>@<script_namespace>/<optional_script_prefix>\"\n",
    "conda_pack_uri = f\"oci://{conda_packs_bucket}@{conda_packs_namespace}/conda_environments/cpu/PySpark 3.0 and Data Flow/5.0/pyspark30_p37_cpu_v5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we need to publish a conda environment with the necessary libraries. To do so from an OCI Notebook Session (recommended), the following cell shows the steps involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "odsc conda install -s pyspark32_p38_cpu_v2\n",
    "conda activate /home/datascience/conda/pyspark32_p38_cpu_v2\n",
    "conda install -c conda-forge requests aiohttp\n",
    "python3 -m pip install -U oracle_ads\n",
    "odsc conda init -b <conda_packs_bucket> -n <conda_packs_namespace> -a <api_key or resource_principal>\n",
    "odsc conda publish -s /home/datascience/conda/pyspark32_p38_cpu_v2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='local'></a>\n",
    "# Establishing a Model Tuning Script Locally\n",
    "\n",
    "We will load an example dataset from ``spark``'s GitHub repo, then train a simple model on it using MLLib and our spark cluster. Ultimately, we'll combine this into 1 script, push it onto OCI Object Storage (along with our conda pack), and run our Data Flow job. ``ads`` will be able to run and watch this job all from the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Source Data\n",
    "\n",
    "We will use the sample libsvm dataset provided by the spark GitHub Repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "with fsspec.open(\"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\") as src:\n",
    "    with open(\"./sample_libsvm_data.txt\", 'wb') as dest:\n",
    "        dest.write(src.read())\n",
    "data = spark.read.format(\"libsvm\").load(\"./sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## Build a Model on Spark\n",
    "\n",
    "The following script will build our MLLib model on a Spark cluster locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "pipeline_model = pipeline.fit(trainingData)\n",
    "\n",
    "predictions = pipeline_model.transform(testData)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = pipeline_model.stages[1]\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to the Model Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running with an OCI NotebookSession and have Resource Principal set up, run the folowing.\n",
    "\n",
    "from ads import set_auth\n",
    "set_auth(\"resource_principal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SparkPipelineModel(estimator=pipeline_model, artifact_dir=tempfile.mkdtemp())\n",
    "model.prepare(\n",
    "    inference_conda_env=conda_pack_uri,\n",
    "    force_overwrite=True,\n",
    "    X_sample=testData.drop(\"label\"),\n",
    "    y_sample=testData.drop(\"features\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we save the model to  our catalog, let's confirm that our score.py runs without errors and that the result is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.asarray([x.label for x in testData.drop(\"features\").collect()])\n",
    "y_pred = model.verify(testData.drop(\"label\"))['prediction'] \n",
    "\n",
    "y_true == y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are ready to save our model to the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before deploying our model, let's confirm we've done the necessary steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And deploy our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"remote\"></a>\n",
    "# Running on Data Flow\n",
    "\n",
    "Now we can put it all together and run our script on Data Flow. The following cell writes all of the code we've gone through above into a single file, script.py. Then it uses ``ads`` to run and monitor this script as a Data Flow Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.jobs import DataFlow, Job, DataFlowRuntime\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "td = tempfile.TemporaryDirectory()\n",
    "local_script = os.path.join(td.name, \"script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {local_script}\n",
    "import pyspark\n",
    "import fsspec\n",
    "import tempfile\n",
    "from ads import set_auth\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from ads.model.framework.spark_model import SparkPipelineModel\n",
    "\n",
    "def main():\n",
    "    print(\"Hello World\")\n",
    "    print(\"Spark version is \", pyspark.__version__)\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    with fsspec.open(\"https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\") as src:\n",
    "        with open(\"./sample_libsvm_data.txt\", 'wb') as dest:\n",
    "            dest.write(src.read())\n",
    "    data = spark.read.format(\"libsvm\").load(\"./sample_libsvm_data.txt\")\n",
    "\n",
    "    featureIndexer =\\\n",
    "        VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "    (trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "    dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "    pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "    pipeline_model = pipeline.fit(trainingData)\n",
    "    \n",
    "    predictions = pipeline_model.transform(testData)\n",
    "    predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "    treeModel = pipeline_model.stages[1]\n",
    "    print(treeModel)\n",
    "\n",
    "    set_auth(\"resource_principal\")\n",
    "\n",
    "    model = SparkPipelineModel(estimator=pipeline_model, artifact_dir=tempfile.mkdtemp())\n",
    "    model.prepare(\n",
    "        inference_conda_env=conda_pack_uri,\n",
    "        force_overwrite=True,\n",
    "        X_sample=testData.drop(\"label\"),\n",
    "        y_sample=testData.drop(\"features\"),\n",
    "    )\n",
    "\n",
    "    model_id = model.save()\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_configs = DataFlow()\\\n",
    "    .with_compartment_id(compartment_id)\\\n",
    "    .with_logs_bucket_uri(logs_bucket_uri)\\\n",
    "    .with_driver_shape(\"VM.Standard2.1\") \\\n",
    "    .with_executor_shape(\"VM.Standard2.1\") \\\n",
    "    .with_num_executors(2) \\\n",
    "    .with_spark_version(\"3.2.1\")\n",
    "runtime_config = DataFlowRuntime()\\\n",
    "    .with_script_uri(local_script)\\\n",
    "    .with_script_bucket(script_bucket_uri) \\\n",
    "    .with_custom_conda(conda_pack_uri)\n",
    "df = Job(infrastructure=dataflow_configs, runtime=runtime_config)\n",
    "df_run = df.create().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the logs and status by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run.watch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Model To Local\n",
    "\n",
    "Finally, we can pull that model locally for further testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "model_reload = SparkPipelineModel.from_model_catalog(\n",
    "    model_id,\n",
    "    artifact_dir=download_dir,\n",
    "    force_overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleanup\"></a>\n",
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the resources from object storage, we need to create a file system instance. The following uses resource_principal, but other auth mechanisms are accepted, such as config. Then call `rm` on each file put into OCI Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocifs import OCIFileSystem\n",
    "\n",
    "fs = OCIFileSystem()\n",
    "fs.rm(conda_pack_uri)\n",
    "fs.rm(os.path.join(script_bucket_uri, \"script.py\"))\n",
    "td.cleanup()\n",
    "download_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "# References\n",
    "\n",
    "- [ADS PySpark Documentation](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/frameworks/sparkpipelinemodel.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)\n",
    "- [Understanding Conda Environments](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/use-notebook-sessions.htm#conda_understand_environments)\n",
    "- [Use Resource Manager to Configure Your Tenancy for Data Science](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/orm-configure-tenancy.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
